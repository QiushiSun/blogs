{"posts":[{"title":"配置LaTeX Beamer的小技巧","text":"分享一些在配置LaTeX Beamer时发现的小技巧 最近为了答辩，魔改了一个有些年份但是自己很喜欢的Beamer模版，因此分享一些在配置中不容易发现的小技巧。 1. 控制封面页的Subtitle如题所示，LaTeX Beamer往往只支持两级标题，但是如果确有需要，可以在subtitle命令内部设置另一个大小字体的文本并且换行达成。可以使用以下命令 1\\subtitle{\\tiny{Knowledge Distillation based on Programming Languages Pre-Trained models and Its Application}\\\\[0.8em] \\scriptsize{中期进度汇报}} 效果预览 2. 优雅地在BibTeX中引用网页一般技术博客或网页不会像Research Gate这样的Paper Repositories一样为你生成Bibtex，因此很多时候需要自己动手。 12345678@misc{T-NLG, author = &quot;Corby Rosset&quot;, title = &quot;Turing-NLG: A 17-billion-parameter language model by Microsoft&quot;, howpublished = &quot;Website&quot;, year = {2020}, month = {3}, note = {\\url{https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}}} How do you get nicely formatted URLs in the bibliography? 更多参考：https://www.kronto.org/thesis/tips/url-formatting.html 3. 如何让caption换行再居中有时候为了美观，在遇到Caption太长需要换行的时候可能会考虑让未填充完一整行的内容居中，但因为Beamer中不常见太长的Caption，因此以Article来举例（二者的使用方法是一样的） 上图为默认的换行方式，可以仿照下面的命令来调整居中，但是因为centering和换行符的优先级问题，因此换行符需要protect 1\\bicaption{\\centering 教师模型在POJ-104数据集和Big Clone Bench数据集微调克隆检测任务的过程}{Performance on the POJ-104 \\&amp; Big Clone Bench Dataset, by Fine-Tuning Teacher Nets on \\protect\\\\ \\centering Clone Detection Task} 4. Beamer 分栏环境下使用脚注在Beamer中使用columns环境进行分栏的话，需要特别注意footnote的位置，为了搞清楚这个footnote到底是属于某个column还是整个page，需要显示标注，以下是一个错误示例（在column中单单使用了\\footnote命令） 使用以下命令显式声明footnote所对应的内容在整张slide的底部出现 12345\\caption{\\scriptsize CNN vs RNN vs Self-Attention vs Fully Connected\\footnotemark}\\end{columns} %分栏环境结束\\footnotetext[1]{$N$代表序列长度，$D$为维度，$K$为卷积核大小}\\end{frame} 5. 调整Beamer插图Caption的大小有时候想要调整一下Caption的大小，但由于大部分页面都会统一Caption比字符小2号，因此一个个修改似乎是治标不治本，而且在caption命令的内部调整偶尔会导致奇怪的错误。查阅手册后发现Beamer和Article一样支持全局修改。 因此，直接使用下面这个全局命令调整吧 1\\captionsetup{font={scriptsize}} 看下将默认大小调整为scriptsize的效果。","link":"/2022/03/22/2022-3-latex-beamer-tricks/"},{"title":"A Structural Analysis of CodePTMs","text":"[论文导读] What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code 简述这篇发表在ICSE 2022的文章来自华中科大，主要研究代码表征预训练模型（Code PTM）的可解释性。通读后发现其中的相当一部分参考了ICLR 2021的一篇对蛋白质Transformer（如TapeBert、ProtBert以及ProtXLNet等，挖个坑之后补）进行注意力机制分析的文章： BERTOLOGY MEETS BIOLOGY: INTERPRETING ATTENTION IN PROTEIN LANGUAGE MODELS 其中的可视化代码，以及Attention Analysis部分，几乎是照搬了蛋白质文章的代码和公式（设置threshold来过滤低注意力值，并计算各个类的注意力分布） 本文大量内容是参考了前人在NLP和生信领域的工作，将它们直接迁移到了Code上。但不失为一种入门Code PTM可解释性这个小领域的渠道。 摘要与要点本文进行了透彻的结构性分析，以期为CPTM提供可解释性，分为以下三个方面 attention analysis: Attention aligns strongly with the syntax structure of code probing on the word embedding: Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer syntax tree induction: The pre-trained models of code have the ability of inducing syntax trees of code. 这些发现可能代表着将code的语法结构融入预训练会帮助得到更好的代码表征 导言本文将当前的代码表征研究方案分为了两类 监督学习模型：任务特定模型，也就是一个编码器网络 (e.g. LSTM, CNN, and Transformer) 为程序生成一个表征向量。 无监督/自监督学习模型：utilize word embedding techniques to represent source code，学习全局词向量，如基于AST的Code2Vec We try to answer the following question: Can the existing pre-trained language models learn the syntactical structure of source code written in programming languages? 核心问题：现在的CPTM能否学习到源码中的句法结构，重申了三点contribution Analyze the self-attention weights and align the weights with the syntax structure，即给定一个代码片段，如果二者有neighborhood关系，那么分配给他们的注意力会比较高，研究揭示了注意力可以捕获源代码的高维结构属性，比如motif structure Design a structural probing approach to investigate whether the syntax structure is embedded in the linear-transformed contextual word embedding of pre-trained code models. 证明了代码的句法结构隐式地嵌入在了模型所学习到的向量中 Investigate whether the pretrained language models for source code provide the ability of inducing the syntax tree without training 说明了预训练模型的确能够在一定程度上学习到源代码的句法结构 作者还提到，本工作和其他以设计更好的网络以获得更好的源代码表示的工作是互补关系。 研究背景Transformers的概述主要是对Transformer的一般架构和计算方式做了简介，包括投影、注意力分数计算、Scaled Dot-Product Attention以及通过positional encodings为输入添加位置信息，没有提出新的概念，对这部分不了解的朋友可以参考下HarvardNLP的The Annotated Transformer这篇文章 Pre-Training Language Model在代码表征领域，最先开这个坑的就是MSRA提出CodeBERT，随后出现了GraphCodeBERT、CodeT5以及PLBART等诸多预训练模型，最近比较新的是ICLR提出的UniXcoder（以后再讲，挖个坑） 值得一提的是这部分是对BERT模型的简介，提及的输入和训练任务都直接照搬了BERT的原文，但代码表征预训练模型的输入和训练任务与BERT有很大不同，以CodeBERT为例，其输入为代码片段和自然语言拼接而成，训练任务为ELECTRA。而GraphCodeBERT基于CodeBERT，在预训练阶段融入了Data-flow信息（通过AST提供） 研究动机早期的NLP工作指出了Transformer的自注意力机制能够帮助捕获一定的自然语言句法信息 上图为经过CodeBERT模型编码的一个代码片段的注意力可视化，（a）图展示了相应的注意力，（b）图展示了Transformer第五层的各注意力头的注意力分数平均值（c）图是这层第12个头的注意力权重，其中标注了一些红色的点为相应ast中有motif structure 本文作者对motif structure的定义是：AST树中包含非叶子节点和叶子节点的结构，可见示例图中（a）图中用深色标注的AST局部结构 示例图中的（b）图展示了自注意力分布的热力图，作者从其中挑选了一些“长方形”，将其归为一个group，结合AST的信息可以发现此热力图所反馈的信息可以和AST相对应，比如tokens在AST的同一个分支。最后（c）图挑选了一个Transformer层中的一个Head做了case study，根据我的经验应该是使用BertViz完成的绘图。 这里有一个重要的概念：motif structure，也就是作者为AST定义了一个特殊的语法结构，它包含了非叶子结点及其子节点，如上图的 (a) 中的if_statement部分。 CodePTMs的结构性分析这部分在文章的第四章，使用了三个不同的结构分析方法，以解释CodeBERT和GraphCodeBERT两个模型，下图为三个方法的示例 注意力分析每个Transformer层中都能获得一套输入的代码的注意力，设计了一个indicator函数𝑓 (𝑤𝑖, 𝑤 𝑗 )，判断两个输入token是否有syntactic relation（句法关联），作者定义了syntactic relation，即 𝑤𝑖 和 𝑤𝑗 在AST中有相同父节点。 公式中的theta为一个阈值，用于过滤掉注意力权重较低（低置信）的token pairs，上面这个公式中相应的注意力仅仅取决于权重的绝对值。因为这些head注意在前一个或是后一个code token，它并不会考虑到code token的内容，因此不会和整个代码片段的句法结构匹配。 （这块的分析简直和BERTOLOGY MEETS BIOLOGY一模一样） 所以进一步调研了注意力的变化性（attention variability），用来衡量对于不同的输入，注意力怎么变化，公式如下 （我觉得这部分完全参考了 Analyzing the Structure of Attention in a Transformer Language Model） 使用单个token pair的注意力分数减去所有注意力分数的均值，只选取了前10个token来保证每个position有足够多的数据，这些位置信息在整个序列中（看起来）是一致的，作者认为变化性的较高则为content-dependent head，较低变化性则为content-independent head Structural Probing on Word Embedding 这部分使用了Structural Probing的分析方法来检查预训练模型是否把句法结构嵌入在上下文中的word embedding中（白话讲就是训练出来的word/code embedding中是否含有句法信息）。该方法的核心概念是（AST的）树状结构在被嵌入一个新的空间后，两个词向量之间的欧氏距离和两个词在语法树之间的边数量对应。 上图的简介图（a）和（c）演示了对代码的Word Embedding的Structural probing 此处学习了一个mapping function，使用代码序列作为输入，模型的每个层生成词向量，然后计算二者在高维空间中的distance。作者认为，如果Vf和Vi的欧式平方距离接近2，那么语法树的结构被较好地保留 参数训练的loss function，两个distance，第一个表示AST中code token的距离，第二个表示词向量的距离，第一个sigma为所有训练序列的平均distance，第二个sigma为代码序列中所有可能的二元组组合，训练目标就是通过误差反向传播更新映射函数B Syntax Tree Induction这一部分研究预训练模型在不训练的情况下引导句法结构的能力 两个code tokens之间的“距离”接近（如注意力分布类似或是表示类似），那么它们应该在语法树中较为接近，比如有公共父节点。此处的假设是，如果源于预训练模型的语法树与gold standard的语法树很相似，我们可以合理的推导出预训练模型保留了句法结构信息这个结论 这里有一个概念，文章没有讲清楚，那就是Gold Standard指代的是什么 我个人的理解：预训练模型得到的token embedding的距离关系和实际AST里两个token之间的距离关系是一致的 通过保证token embedding的距离关系来验证没有通过树结构就学到语法了 ![Visualization of attention heads in CodeBERT](/gallery/2022-6-12-Code-Capture/Capture-Paper-Vis-of-att-heads-in CodeBERT.png “Visualization of attention heads in CodeBERT”) 实验设计和结果作者在这里以三个Research Question的形式展示了实验的效果 RQ1: Attention Analysis这张图展示了CodeBERT和GraphCodeBERT的注意力是如何与AST对应的，其中的色块是AST中连接了motif structure高置信度的注意力权重的proportion，柱状图展示了每个层的最大值 RQ2: Probing on Word Embeddingprobing中的python语言的斯皮尔曼系数 RQ3: Syntax Tree Induction 上图为一个针对CodeBERT模型的Case study，比较了是否注入了bias，从图中可以看到CodeBERT能捕获多个motif structure，说明了syntax tree induction的有效性 本节小结： 代码的语法树可以由代码表征预训练模型induce得到 从注意力信息中抽取语法树比从CodePTM的上下文表示中抽取更有效 Limitations and Future Work进一步的研究 预训练的另外几个方面 Control-flow graphs (CFGs) Data-flow graphs (DFGs) 更多预训练模型 本文只研究了CodeBERT and GraphCodeBERT的表征，言下之意：这两个模型都是Encoder-only的模型，其他架构的模型如PLBART，CodeT5也可以整一发，还有一些经典模型，比如把LSTM和CNN + 注意力模型应用在代码的处理上也可以使用类似的分析方法。 Structural Analysis 本文的Structural probing相对简单，用一个更深的网络也许可以获得更好的映射 树结构 所涉及的Right-skewness bias是用于NLP的，或许可以使用AST结构来补充 回顾一下，本文从三个角度对代码表征预训练模型进行了分析，把一些NLP中的可解释性分析方法迁移到了Code领域，并且融合了生信领域的一些工作。不过本文并没有基于这些发现改进模型（比如给Code的各种下游任务提点） 后记补充一下注意力权重的可视化策略，之前提到过这篇文章的注意力可视化可能用了BertViz，我个人也觉得BertViz非常的好用，Github给的demo可以支持交互效果，而且如果报告/文章版面够大的话，还可以这么玩（选择neuron view）","link":"/2022/06/12/2022-6-12-Code-Capture-Paper/"},{"title":"Megatron：使用张量并行训练超大Transformer","text":"[论文导读] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism Megatron：使用张量并行训练超大Transformer","link":"/2022/06/30/2022-6-30-Megatron/"},{"title":"快速为 Hexo-Icarus 配置 MathJax（2022）","text":"如何让自己的博客能可靠地渲染数学公式？这是个问题。 前言前几天在将hugo博客迁移到Hexo-Icarus后，发现了MathJax无法正常工作的问题。在搜索引擎搜索的话，会得到一些类似的答案，尤其是csdn上的答案，不仅不完全正确，而且因博主们持续不断地互相抄袭，这些答案早就不适合2022年的hexo-icarus了，真正可靠的答案很少，而且都被淹没在了这些垃圾信息中。 解决方案大多数人刚配置好Hexo-Icarus后会出现LaTeX Error，典型的情况如下图所示 还有一些个别情况，会出现公式直接被吞无法显示的情况，一般是有些LaTeX符号在Hexo生成页面时，被识别成Markdown语法而被渲染成HTML标签导致的，这也不用急，先尝试排查一下npm modules看一下自己装了哪些包，然后卸载掉安装好Icarus主题后再安装的公式package，Icarus自带行间公式的渲染。 如果发现自己有hexo-math，还需要卸载这个package 1$ npm uninstall hexo-math 使用以下命令，换装hexo-filter-mathjax，在Hexo端渲染MathJax 12$ npm install hexo-filter-mathjax$ hexo clean 在编辑博客md文件时，在header区域加上 mathjax: true，如下所示 123456789title: \"快速为 Hexo-Icarus 配置 MathJax（2022）\"date: 2022-07-17 21:00:03mathjax: truecategories:- Blogtags:- Blogkeywords:- Blog 然后重新使用 hexo g 命令生成博客页面，再使用 hexo s 进行预览，会发现问题解决了。 效果行内公式CodeBERT的输入形式为 ，第一段为自然语言文本，第二段为代码，训练的数据可分为两种，即bimodal data，即NL-PL Pairs和unimodal data，也就是纯代码。 行间公式行间公式使用{raw}...{endraw}进行包裹，如下 12345{% raw %}$$\\begin{equation}M_{i j}= \\begin{cases}0 &amp; \\text { if } q_{i} \\in\\{[C L S],[S E P]\\} \\text { or } q_{i}, k_{j} \\in W \\cup C \\text { or }\\left\\langle q_{i}, k_{j}\\right\\rangle \\in E \\cup E^{\\prime} \\\\ -\\infty &amp; \\text { otherwise }\\end{cases}\\end{equation}$${% endraw %} 以上就是在为Icarus配置MathJax时的一些经验分享，有时候直接去这些主题仓库的issue中寻找答案，会比用搜索引擎的效率高很多，希望能有所帮助。","link":"/2022/07/17/2022-7-17-Icarus-MathJax/"},{"title":"ZeRO：切片的数据并行","text":"[论文导读] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models “零冗余的优化器”Zero Redundancy Optimizer","link":"/2022/07/03/2022-7-3-ZeRO/"},{"title":"Code预训练语言模型学习指南（原理&#x2F;分析&#x2F;代码）Part2","text":"Code预训练语言模型学习指南（原理/分析/代码）Part2 这个主题的上一篇文章 Code预训练语言模型学习指南（原理/分析/代码）Part1 讲解了几个典型的Code预训练语言模型（CodePTMs），这篇文章顺延这个话题，继续讨论21年及以后出现的CodePTMs，并且补充一些代码。 PLBARTNAACL 2021 Unified Pre-training for Program Understanding and Generation 顾名思义，就是应用于编程语言的BART，参考了ACL 2020的文章：BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 先简单说下BART，它吸收了BERT模型的双向编码器和GPT中的单向left-to-right解码器这二者的优点，因此比BERT更适合文本生成的场景（BERT因Pre-training阶段和生成式下游任务差异比较大，因此被认为不适合做NLG相关任务），而它相比GPT，也多了双向上下文语境信息（GPT是单向建模）。除此之外，相比BERT的Token Masking，BART对Encoder端采用了更加复杂的Noise。 基于对BART的知识，理解PLBART并不算困难，其使用了和BART-base相同的架构，唯一结构上的不同的是它在Encoder和Decoder的顶部添加了一个额外的LayerNorm。在Noise策略方面，PLBART使用了token masking, token deletion和token infilling这三种策略，与BART相比少了Sentence Permutation和Document Rotation这两个任务，这些任务的细节都可以参考BART原文。 在微调下游任务方面，作者将PLBART的下游任务分为两块 首先是Sequence Generation，又可细分为三个任务，可参考下图 其次是Sequence Classification：将序列尾部的special token喂给线性分类器用于预测，与BERT等模型的分类区别不大。 实验与比较方面，作者先指定了baseline模型，并将其分成了两种 Training from Scratch，作者用下游任务的数据集从零开始训练了LSTM+Attention以及一个TransformerPre-trained Models，作者挑选了RoBERTa、CodeBERT、GraphCodeBERT、GPT-2、CodeGPT(-adapted)具体的实验部分做了Code Summarization、Code Generation、Code Translation这三个生成式任务，效果自然是好的，在Classification方面做了两个任务：clone detection和vulnerability detection，在后者上PLBART不如基于AST的模型。 CodeT5EMNLP 2021 CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation 文中对CodeT5的描述是：a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers，即一个能更好地利用代码语法信息（形式是identifier，即标识符）的统一预训练Transformer模型。在开始之前，和PLBART一样，先简单说下Google T5模型。T5的名字来源是Text-To-Text Transfer Transformer，顾名思义T5把所有的NLP问题统一归纳为了Text2Text任务，用来做NMT、QA、文本摘要和文本分类任务。 对比下T5原文，可以发现二者的核心思想还是非常类似的，作者将CodeT5归纳为a pre-trained encoder-decoder model that considers the token type information in code，细心的玩家可能发现了，前面提到的CodeBERT为首的BERT类模型和CodeGPT为首的GPT类模型，仅含有Encoder或Decoder，而非完整利用一个Transformer架构来处理代码。因此CodeT5最大的卖点即第一个unified encoder-decoder CodePTM，可以理解为完全使用了Transformer的两个部分。 此外，除了使用T5的架构外，作者使用了以下两个方案来更好地利用代码结构特性： 使用了代码的标识符信息，提出了Identifier-aware Pre-training，是一个与T5中Masked Span类似的目标，简而言之就是随机mask掉任意长度的Span，然后让decoder去预测。利用了代码地Comments（自然语言注释）信息，作者称之为Bimodel Dual Generation，让自然语言和源代码的表征可以对齐，帮助缓和Pre-training和Fine-Tuning阶段的差距。 文章发表时在CodeXGLUE Benchmark的若干任务上取得了SOTA效果。 UniXcoderACL 2022 UniXcoder: Unified Cross-Modal Pre-training for Code Representation 这是当今（2022.7）的SOTA模型，参考了NIPS2019: Unified Language Model Pre-training for Natural Language Understanding and Generation 文章选择了五个任务，分为以下三类 代码理解任务：Clone Detection和Code Search代码生成任务：Code Summary和Code Generation自回归任务：Code Completion本文很重要的一个卖点就是更全面地利用了AST提供的代码结构信息。文章开头讲过，AST一般会被表示为一个Tree结构，不能直接作为Transformer类模型的输入，回忆一下前面提到的GraphCodeBERT，作者在以损失相当一部分信息的情况下让模型学习AST的数据流。为了能更加有效地利用AST，因此UniXcoder地作者构建了一个one-to-one的mapping function，将AST转为一个序列结构（flattened token sequence），然后和Code Comments一同编码，对这个mapping function的有效性的证明在文章的附录中。 模型结构方面，UniXcoder的一个卖点就是一个统一的，可以同时兼容Encoder-Only，Decoder-Only和Encoder-Decoder三种模式的CodePTM，相当于给模型添加了“开关”，来决定采用什么模式处理任务，用白话讲，就是通过使用三种不同类型的自注意力Mask策略来控制模型的行为。 既然同时能拥有三种模式，那么自然会有更多预训练任务，如下所示： Masked Language Modeling（MLM），算是基本操作了。Unidirectional Language Modeling（ULM），用于训练decoder-only模式，帮助完成自回归任务，对应的是右上三角masking。Denoising Objective DeNoiSing (DNS)，可参考BART和T5，用于训练encoder-decoder模式，帮助完成生成任务，参考架构图中的encoder-decoder部分。除了上面这些任务以外，作者还提出了Code Fragment Representation Learning 其中包含了Multi-modal Contrastive Learning (MCL) 和 Cross-Modal Generation (CMG)这两个任务。前者采用了一个对比学习损失，后者是使用了一个统一的自然语言描述（comment），文中使用了fulcrum，即支点这个词，让模型学习到的代码表征在不同语言之间的对齐。 还需注意的一点就是，UniXcoder在预训练和微调这两个阶段中的输入形式有所不同，由于引入了Flattened AST，AST展开后的序列中被引入了大量额外的tokens（70% longer）会导致额外的开销。因此，在微调阶段UniXcoder仅使用AST的叶子节点，为了缓解这个gap，在预训练阶段作者设置了0.5的概率随机丢弃输入序列中的非叶子节点。 除了Clone Detection、Code Search、Code Summarization和Code Completion等任务上表现较好外，UniXcoder还提供了一个新任务：zero-shot code-to-code search，即在zero-shot的情境下，通过一个源代码的query在candidates集合中寻找语义相同的源代码，该任务使用的数据集是CodeNet，2021年的一篇数据集文章，用来衡量训练所得的code fragment embeddings的效果。 相关代码整理CuBERThttps://github.com/google-research/google-research/tree/master/cubert CodeBERT、GraphCodeBERT和UniXCoderMSRA提供了CodeBERT、GraphCodeBERT和UniXCoder在下游任务微调时可用的代码，在仓库https://github.com/microsoft/CodeBERT，但没有提供预训练阶段的实现（CodeBERT和UniXCoder在预训练阶段都使用了16 张 32GB NVIDIA Tesla V100实现），使用时通过transformers加载checkpoints就可使用。此外，huggingface还提供了一个经济适用版的CodeBERT模型：huggingface/CodeBERTa-small-v1 CodeGPT与上述三个MSRA提供的模型一样，CodeGPT仍然是提供了可通过transformers加载checkpoints，即CodeGPT-small-java-adaptedGPT2 CodeT5https://github.com/salesforce/codet5 PLBARThttps://github.com/wasiahmad/PL 总结上述对CodePTMs相关的内容大致上是六月中下旬赶一个文章时调研文献后总结的笔记，然后补充了点链接和细节，实验部分写的比较简略，是因为如果每个模型的实验都要全讲的话篇幅就太长了，而且这些任务都大差不差，每个模型都讲一遍冗余会比较多，之后可能会在其他文章补充。此外，有一些影响力比较小或者Task-specific的工作可能没完全覆盖到。总而言之，不论是CodeBERT开的新坑还是今天的SOTA模型UniXcoder，MSRA在这个领域还是完全dominant的存在。对于CodeBERT和GraphCodeBERT为首的大模型，复现预训练阶段的成本很高，不适合平民玩家，而且今年五月的IJCAI 22 Survey Track连CodePTMs的Survey工作都已经出了（Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code），可能短时间内出革命性的新模型的可能性不大，而且Code领域使用的这些方法终究还是跟着NLP走的，需要NLP提出新技术后Code领域才有跟进的可能。个人感觉接下来在这个相对较小但很卷的领域的研究热点可能会慢慢向可解释性和模型分析（Analysis of Models &amp; Interpretability）方面转移，最近还研读了一些22年新出的Probing CodePTMs的文章，之后再补充。 希望本文对想熟悉代码表征预训练模型这块的朋友有所帮助。","link":"/2022/07/09/2022-7-CodePTMs-Review2/"},{"title":"Code预训练语言模型学习指南（原理&#x2F;分析&#x2F;代码）Part1","text":"Code预训练语言模型学习指南（原理/分析/代码）Part1 Introduction自从2020年CodeBERT开了代码表征预训练模型（本文称之为CodePTM）这个新坑后，在短短两年的时间内出现了若干个程序设计语言（Programming Language，称之为PL，与Natural Language，也就是NL对应）语言模型。它们的共同特点是大部分用于处理PL（或Source Code）时所采用的技术是从NLP中的Transformer-based模型迁移而来，但这些CodePTMs又各有特点，从训练的角度出发即不同的预训练/微调策略，从数据的角度出发，比如是否使用了代码的结构信息，即抽象语法树（Abstract Syntax Tree，本文将其简称为AST）及其变体。而从架构出发，这些Code预训练模型大致可以被分为以下这三类： encoder-only：CuBERT、CodeBERT、GraphCodeBERT等 decoder-only：CodeGPT、GPT-C等 encoder-decoder：PLBART、CodeT5等 本文对各个CodePTM建模编程语言的思想进行回顾，并简要分析了一下它们的特色。对文中提到的所有CodePTMs的描述主要从背景、预训练策略、微调策略以及下游任务这几个角度出发进行分析，考虑到这些模型之间都存在一些共性以及文章篇幅原因，文中略去了一些通用的处理手段和细节，因此对各部分的分析讲解详略不一，不过都保留了建模编程语言最核心的思想。阅读前需要对Transformer有一定的了解，考虑到单篇博客的长度，这些模型分在两篇博客中讲完。 CuBERTICML 2020 Learning and Evaluating Contextual Embedding of Source Code CuBERT，即Code Understanding BERT，和后面提到的CodeBERT可被归为同一个时期的工作，虽是首个提出Code预训练模型的工作，但和CodeBERT相比，其影响力较小（我在写这篇文章的时候CuBERT引用还没过百），具体原因个人认为是它仅对Python语言进行建模（CodeBERT同时对6种编程语言建模），且它的下游任务和CodeBERT相比不太丰富，主要是以代码理解任务为主。 作者通过GitHub采集了7.4M Python语言编写的程序用于CuBERT的预训练，将基于Transformer类模型处理自然语言的手段迁移到了编程语言上，使用的模型架构和训练方式直接照搬了BERT-Large（24层16个注意力头），然后使用了一个处理过的ETH Py150数据集进行微调。 与此同时，作者还训练了一组Word2Vec embeddings、Bi-LSTM模型和Transformer模型用于比较。 就任务而言，作者构建了一个Benchmark，其中包含了： 五个分类任务（具体细节和描述可参考原文附录） Variable-Misuse Classification Wrong Binary Operator Swapped Operand Function-Docstring Mismatch Exception Type 一个定位+修复任务（Variable-Misuse Localization and Repair），也是本文唯一一个非分类任务 就这几个下游任务而言，可以看到CuBERT主要还是在做代码理解领域的判别式任务，与后续出现的CodeXGLUE Benchmark比其在任务的数量和类型上都有局限性。而且，由于它仅采用了Python语言，和后面出现的各种CodePTMs比局限性比较大，因此仅做简单的科普。 CodeBERTFindings of EMNLP 2020 CodeBERT: A Pre-Trained Model for Programming and Natural Languages 相比前面提到同期工作的CuBERT，CodeBERT的影响力比它大很多。一方面是因为它是多语言（multi-programming-lingual）模型，纳入了6个编程语言，另一方面是它和MSRA自己的CodeXGLUE Benchmark配套后在各个下游任务上被广泛使用。 除了预训练阶段的任务有变化外，CodeBERT的其他方面与自然语言中的BERT模型训练基本无异（其本质上的一个RoBERTa），CodeBERT使用了bimodal data（即PL-NL Pairs）进行了预训练，预训练数据的来源为CodeSearchNet数据集，其中有Python, Java, JavaScript, PHP, Ruby和Go这六种编程语言的2.1M bimodal data和6.4M unimodal codes（也就是没有对应comments的纯代码），这些数据的来源都是GitHub中的开源仓库，并且后续的很多工作也在预训练阶段用了CodeSearchNet数据集。 CodeBERT的输入形式为 ，第一段为自然语言文本，第二段为代码，训练的数据可分为两种，即bimodal data，即NL-PL Pairs和unimodal data，也就是纯代码。 **Masked Language Modeling (MLM)**，算是Transformer类模型的预训练中最老生常谈的任务了，作者将其应用于基于bimodal data的训练 **Replaced Token Detection (RTD)**，迁移自ELECTRA，既可以利用bimodal data进行训练，还可以进一步利用unimodal data（比如是没有对应自然语言文本的code），具体细节可以参考ELECTRA原文。 实验部分做了Natural Language Code Search，个人认为文中没有添加更多下游任务是受到EMNLP的篇幅限制，使用CodeBERT可以在多个下游任务，如Clone detection（克隆检测）、Defect detection（缺陷检测）、Code summarization等上得到出色的结果，具体可参考CodeXGLUE，如下图所示 从CodeBERT开始，后续的CodePTMs就全部继承了对多个PL的支持，不过CodeBERT完全使用了建模自然语言的手段来为Code（或是说NL-PL Pairs）做预训练，忽视了代码的一个很大的特性，那就是结构信息，具体而言就是在编译器进行语法分析阶段生成的抽象语法树（Abstract Syntax Tree，本文称之为AST），紧跟着CodeBERT的GraphCodeBERT立刻填上了这个坑。 GraphCodeBERTICLR2021 GRAPHCODEBERT: PRE-TRAINING CODE REPRESENTATIONS WITH DATA FLOW 看名字就知道这是CodeBERT的后续工作，主要想法就是为CodeBERT添加代码的语法信息，使CodePTM可以显式学习代码的结构信息。 GraphCodeBERT基于数据流学习代码的表征，如下图所示 数据流的获得分为以下几个步骤 通过语法分析工具获得AST，原文中使用的工具是tree-sitter 从AST中提出变量，构成一个由变量组成的序列 从AST中抽取变量之间的依赖关系，文中称之为“value comes from”，构造数据流图 GraphCodeBERT在模型预训练阶段额外提出了两个在当时较为新颖的训练任务 Edge Prediction，即数据流图边预测，通过预测数据流图的边学习代码的结构信息 Node Alignment，即变量对齐，具体而言是学习数据流图中的某个node来自输入代码中的哪个code token 将它们和从CodeBERT（或是BERT or RoBERTa）继承下来的MLM任务一起优化。考虑到AST是一种图结构，为了让Transformer能适应其与一般序列结构的差异，作者修改了其注意力机制，主要是通过调整Attention Mask缩小感受野。 作者将其称为Graph-Guided Masked Attention，其中E代表的是数据流图的边，E’代表的是数据流图的节点和代码的对应关系边。 就下游任务而言，GraphCodeBERT文中主要完成了Natural Language Code Search、Clone Detection、Code Translation和Code Refinement这几个任务，它同样适用CodeXGLUE Benchmark中的其他任务，比如Code Summarization等。 GraphCodeBERT相较于前作CodeBERT解决了CodePTM只学习自然语义，而不学代码结构/语法的问题。但细心的读者或许能发现，学习数据流相较于学习AST本身有相当的信息损失，这也为之后的UniXcoder挖了一个小坑。 GPT-CFSE/ESEC 2020 IntelliCode Compose: Code Generation using Transformer GPT-C是为了代码补全（Code Completion）这个任务而设计的，作者认为之前的的代码补全工作有两点不同 根据上文的token来预测下个token，没有将代码的全文环境纳入考虑 多语言效果不佳 作者提出的GPT-C是GPT-2模型的变体，在一个大规模、无监督、多语言的数据集上从零开始训练。基于GPT-C，作者构建了一个代码补全Framework，称之为IntelliCode Compose，并对多种编程语言进行建模。作者将Sequence decoding的过程视为对树的搜索，搜到出现目标token为止。 虽说是多语言，但是使用的是Python, C#, JavaScript 和 TypeScript，和CodeXGLUE不同且少了两个语言。 就多语言模型的训练而言，作者提出了四个训练的策略 Language-agnostic baseline，即忽略掉编程语言的不同构建一个baseline多语言模型。 Language-type embedding，即加入一个向量来表示每种编程语言，和token embedding等相加。 Language specific control codes，每个输入的训练样本前拼接一个”lang ∗ remaining token sequence”字符串，∗即为编程语言 add a programming language classification task during model pretraining，即在预训练阶段加入一个分类编程语言的任务 关于IntelliCode Compose：作者将Sequence decoding的过程视为对树的搜索，搜到出现目标token为止 在文末，作者考虑到了模型的推理开销问题，还上了一个知识蒸馏，并且还讨论了一下模型基于K8S和VS Code的部署问题。 Code-GPTCode-GPT是在CodeXGLUE中被提出的，没有单独成文，不要和GPT-C搞混了。作者实现它的目的是为了code completion 和 text-to-code generation任务。它就是一个由Code训练，与GPT-2完全同架构的12层Transformer Decoder模型，不过MSRA的研究者实现了两个版本 Pretrained from scratch：随机初始化，从零训练 CodeGPT-adapted：先使用一个GPT-2作为起始点，再持续使用Code进行训练，作者将这个方法称为“domain-adaptive” 更详细的内容可以参考CodeXGLUE原文的4.2节，作者在Huggingface提供了CodeGPT-small-java 和 CodeGPT-small-java-adaptedGPT2 这两个checkpoints，正常地使用transformers库加载就能使用了。 以上是5个代码表征领域的预训练模型的简介与简要分析，还有若干模型会在下一篇博客中继续讨论。","link":"/2022/07/06/2022-7-CodePTMs-Review1/"},{"title":"ViLT：图像-文本多模态Transformer","text":"[论文导读] ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision 前言与背景背景ViLT是ICML 2021的一篇Oral Presentation，它提出了一个极其简单的多模态训练方法，大大减少了视觉-文本多模态任务中处理图像特征抽取部分的计算量，将主要的计算量都放在了模态融合上，显著提高了模型的推理的速度，把方法的建模做了相当程度的简化，算是多模态领域这两年来里程碑式的工作了。 主要贡献ViLT把目标检测（Object Detection），也就是文章Title中提及的Region Supervision从多模态任务里面去掉了。天下苦目标检测久矣，回忆一下DETR就是因为把目标检测框架做的简单，不仅可以End2End（端到端）训练，而且支持后处理。而ViLT直接把目标检测这个高开销任务干掉了，让模型明显简化和推理加速。 从上图可以概览一下前面的工作，文本的处理都很简单，通过word embedding生成向量，扔到Transformer里就可以了。但是视觉这边就不一样了，对于最流行的区域特征，先给一个图像，通过一个Backbone网络，然后RoI，相当于就是做了一次目标检测（抽取Bounding box），所以图像处理的结果就跟文本的单词一样，都是离散形式存在的（也就是一个序列），最后再把文本和图像的序列扔给Transformer进行处理。把图像化为和文本序列一样的离散化表示是一种理论上可行且实现方便的方法，这也就是为什么之前的工作都是把和文本一起训练的通过OD来建模。但是，整个模型的运行时间分配并不合理，用BERT类模型处理文本只占用约15ms，但是视觉OD部分占了810ms。过去这几年（ViLT出现之前）一直有各种各样的工作把视觉这部分的时间压缩，但并没有根本性解决这个问题的方案出现。 而ViLT直接把这件事情反过来了，视觉部分只占据0.4ms，也就是说处理图像部分的时间开销反而只有文本的1/35了，获得了上千倍的运行时间的减少，这也是本文最大的卖点。但实际上，之前被诟病的高开销目标检测也并没有这么不堪，因为可以复用一些处理好的、静态的数据（抽取好的特征），而ViLT虽然推理很快，但是其训练依然是很昂贵的，需要64张32g V100训练三天，它甚至比之前的方法训练时间都要久，而且从上图也可以看出其性能和基于RoI的模型比并没有显著提升。因此，个人认为ViLT最大的贡献是开了“Without Convolution or Region Supervision”这个坑，而非是模型的性能。 接下来对paper本身进行分析 文章解读TitleViLT: Vision-and-Language Transformer Without Convolution or Region Supervision 解释一下这里的Conv和Region Supervision的具体含义 卷积：指的就是预训练好的一个模型抽出的backbone，提供特征图。 区域特征：简而言之就是目标检测的框框。 可以看出，作者最希望体现的卖点，就是既不用卷积带来监督信号，也不用区域特征带来监督信号，以此降低计算复杂度。 AbstractVLP（视觉文本预训练）在过去几年发展的很好，在各种downstream任务都有很好的效果，但是这些模型在图像处理部分花费的精力太多了。常理而言，视觉部分越复杂，开销越大，结果越好，因此之前的工作太过依赖于feature extraction，也就是看为一个目标检测，模型又有Backbone网络在工作，又有Region Supervision在工作。这篇文章针对这个点开始研究，指出了一下两个问题： 效率不行，抽特征花了太多的时间，花费的时间已经超过了模态融合，对下游任务不友好。 如果只用一个预训练好的模型去抽取特征，模型的表示能力肯定有问题，比如用一个目标检测器，因为目标检测的数据集规模都不大，不能涵盖问题的边边角角，因此如果只是抽特征，大概率就不是最优解。而且深度学习时代，总得想办法做End2end的学习。 因此，ViLT提出了极简化的方案，图像预处理做的和文本预处理一样，都通过linear embedding就结束了，正是因为这种设计，ViLT的速度快了很多，但是性能依旧保持竞争力。 Introduction模态学习近几年很火，进步也很快，因为也采用了PT-FT的方式。不论是nlp还是视觉，大家现在都这么干，当把它拓展到多模态的时候，效果也不错。作者指出在这个条件下，预训练阶段的学习比较重要，它不仅能够为模型提供一个好的初始化，甚至还可以帮助模型Zero-Shot做下游任务，催生了多模态的视觉文本联合预训练模型。 在文章的导言部分，作者罗列了2019-2021年间出现的若干引用过百的文章（显然，背后还有上千篇跟进性质的工作），可见这个东西过去几年发展的速度多快，19年Vilbert、VisualBert、VLBert三篇文章名字很像。因此多模态虽然听起来还是挺新的，但实际上竞争很激烈，需要拼手速。 作者认为，一般来说上述模型都是图像文本对来训练的，他们的目标函数都是imgae-text matching为主导的，也就是计算匹配loss和nlp专用的掩码学习，作者在footnote中指出了，虽然还有其他的目标形式，但是imgae-text matching和mlm基本是所有模型都要使用的。同样的，ViLT也是用了这两个任务。下游任务涉及到自然语言和视觉这两个模态，不过要注意图像和文本的数量关系并不是一一对应的，可能是一对多，也可能是多对一。从宏观上看过以后，我们看看ViLT的具体内容： 首先，如何去处理文本？ 这个问题的解决方案比较统一，17年开始学界就是用Transformer处理文本了，而且也早早一统江湖，文本可发挥的地方不大。 重要的是，是如何处理视觉部分 视觉部分就有很多可以发挥的地方了，你的图像的像素，必须被转化为某一种形式，比如离散化，能够跟language tokens去匹配起来，然后才能和文本一起喂给Transformer去玩Self-Attention。你可能会问为什么不能用Pixel？ViT已经指出了这个问题，就是长度太长了，Transformer接受不了，因此ViT把图像分割为16*16的patches，用patch构建的特征喂给Transformer。因此，就是这类问题的核心就可以抽象为像素 → 语义特征的处理方法如何优化了。 作者指出了，大部分这种VLP都依赖一个目标检测器，原因归纳如下 想要一个离散化的特征，目标检测的框就是天然的离散特征，给你图，返回我框，而且这些框有明确的语义信息，直接用RoI抽取特征即可。思路打开，可以把每个这样的Bounding Box理解为一个词语。 下游任务，因为对于Vision-Language的任务而言，要么是VQA，要么是Captioning，他们和物体的联系太紧密了，对于Visual Reassign/Retrieval来说也是，其实你只要正确地识别出了物体，就有了很大的概率来匹配到相应的文字 目前的VLP都是用了预训练好的目标检测器（Visual Genome数据集，1600个类别和400个属性），就是看中了覆盖类别数量多。如果是coco的80类，很有可能就超出匹配的空间了，总而言之类别越多越好。此外，各个工作也都需要在同数据集比较，因此都选择了这个VG训练的目标检测器，但是它非常昂贵。 作者认为之前大部分工作还是集中在提升视觉编码器上，学术界关注效率没有关注性能这么火，尤其是多模态，都不把它当回事，因为在训练的时候可以抽取好目标检测的特征，放在硬盘上，需要的时候从里面拿，因此可以发现抽特征可以在线下完成，真的训练没想象中这么贵。但是！真实世界中，你的数据每时每秒在新生成，你每个时刻都要去抽取特征，这个时间就不能忽略了，这就是前面工作最大的局限性。 对先前工作的归纳这篇文章比较神奇的地方就是：它的背景知识介绍比自己的方法长得多，作者把抽取图像的技术路线写了一整页，重新定义了vision-language model，并按照以下策略进行分类。 图像和文本的表达是否平衡：使用参数量和计算量来衡量 模态之间如何交互与融合 先对Abbreviation进行一下简单的解释 VE：Visual Embedding TE：Textual Embedding MI：模态融合 对于这四张子图而言而言 文本轻量，视觉很贵，融合轻量（点积 or 浅层网络），其代表为Visual Semantic Embedding (VSE) 二者开销基本等价，但融合轻量（点积），其代表为CLIP 最主流的方法，基本80%的工作都是这个形式，文本处理便宜，目标检测非常昂贵，但是性能还不错，其代表为Visual-BERT和UNITER 作者提出方法，认为在多模态任务中，两个模态的融合比较重要，和特征关系最好不要这么大，希望二者的特征抽取都能便宜一点 ViLT的模型结构模型的输入细节：： Word：L*H 图像（Patch Embedding）：N*H 灰色指代模态，0就是文本，1就是图像，这是因为concatenate后，模型并不知道谁是谁，不利于学习。因此你要告诉模型哪块是文本，哪块是图像，模型的输入和输出等长度。 其中的✳分别代表文本和图片的[CLS] Token 注意：图像中的灰色-绿色-浅绿/灰色-紫色-浅紫，也就是文本/图像的Model-type Embedding，Token Pos Embedding和文本/图像本身的Embeddeding看起来是拼接起来的，实际上是加法加在一起的。 训练的策略与细节 Image Text Matching：人为设计的任务，故意把和文本匹配的图片随机替换掉，实际上做二分类，让模型判断一张图是否被替换为了另一张图片。 MLM：NLP经典任务，都懂的。 Word Patch Alignment：文本和图像的相似度如何？OT是把几何和概率联系起来的一个工具。可以理解为在计算文本和图像概率分布的distance 看到这里可能有个问题：为啥不给图像也来个MLM？当时ViT已经有了，但是MAE还没出来，不过MAE现在（22年）已经很火了，就前面的模型架构图来看，把图片中的紫色部分遮住，似乎就是个BERT，把蓝色部分遮住，似乎就是个ViT，因此在图片的Caption中也说了是受到了ViT的启发。 Whole Word Masking：作者举了个例子，这个giraffe，被劈成小的词根（token）以后，你mask了中间一部分后，由gi和raff开头结尾的单词并不多，很容易就能猜出来是giraffe，根本就不学了。这就导致了你做多模态的时候我压根不借助图像信息了（short cut），一部分loss就失去了意义，因此还不如全部mask掉，让模型一定要借助图像信息。 Image Augmentation：之前的多模态任务里没办法用数据增强的，因为特征都已经抽取好了，只要一增强，特征就要全部重抽。作者这里既然已经end2end了，直接就上了修改版的random augment，用了ra的policy，但是去掉了color gitter和cutout，让文本能够和图片对应起来。 ViLT所使用的数据集 MS-COCO：每个图对应5个Caption，标题较长（平均超过十个词） VG：10w图，对应500w文字描述，但是标题比较短 GCC：一对一 SBU：一对一 一般将他们称为4-million，很多人沿用了这个设定，但实际上每个数据集有自己的特性，比如一对多和多对一关系。因此在衡量数据集大小的时候，我们一般是用图片的数量来衡量数据集的大小，而非是有多少个pair。注意：GCC和SBU现在下不完整了，有很多url失效了。 实验效果分类任务 时间：ViLT最大的卖点，之前都是900ms，即使Grid feature也有100ms，但是ViLT直接降低到了15ms 结果：依然是基于目标检测（区域特征）的模型效果最好，但ViLT的速度和精确度结合比较好（成功Tradeoff） Retrieval 上面是Zero-Shot的性能，下面是经过Fine-Tuning的性能，总而言之，ViLT的性能还差一点点（这样才有坑可填），但性能并不是它的卖点，所以也不用太过在意。 消融实验作者就训练步数对模型性能的提升和前面提到的训练技巧进行了消融。 问题1：训练时间 很多自监督的工作，都说自己训练越长越好，比如MoCo，SimCLR，MAE（1600epochs），作者尝试了下这个东西的steps对性能是否有影响。得出的结论是有提升，但是提升很少，消融应该就是在25k做的，但是刷分肯定挑了更好的模型。 问题2：前面的技巧 证明了ViLT所使用的图像数据增强非常有效，而且注意，作者在这里让图像也简单的做了下完形填空，但是效果不怎么样（当时MAE还没出来）。 拓展方向 Scalability：Transformer都是越大越好的，数据越多越好，来个ViLT-H 图像的遮蔽语言建模：今天看来，已经被MAE做掉了，而且有超级多的跟进工作 更先进的数据增强：消融就能发现，数据增强很管用，而且能提高Data Efficiency 总结本文的写作有些非主流，背景知识写出了文献综述的味道，而且是自己方法的两倍长度。不过个人觉得本文对之前工作做的Taxonomy很好，重新定义了vision-language model，来彰显自己方案的可靠性。从文章真正的贡献而言： 模型既没有贵的Embedding，又能保住性能，简单且有效（也让后面人有坑可以填） 证明了不再需要Conv或者Region，任务一样可以做好 不过需要注意，ViLT是用PyTorch-Lightening写的代码，和原生PyTorch有区别，而且需要64个V100训练三天，似乎99%的从业者都不能承担。 总而言之，ViLT算是一个多模态领域开创性的工作，成功地平衡了训练/推理时间开销和下游任务的性能，因此也出现了若干跟进性质的工作，在之后的文章导读中应该还会继续涉及这个话题。","link":"/2022/08/11/2022-8-10-ViLT/"},{"title":"论文导读：Code Transformer","text":"[论文导读] Language-Agnostic Representation Learning of Source Code from Structure and Context","link":"/2022/08/18/2022-8-17-Code-Transformer/"},{"title":"论文导读：Probing Pretrained Models of Source Code","text":"[论文导读] Probing Pretrained Models of Source Code 本文为对一篇代码表征预训练语言模型（CodePTMs）的可解释性领域的文章：Probing Pretrained Models of Source Code 的解读。","link":"/2022/08/19/2022-8-19-Probing-CodePTMs/"},{"title":"[2022] 配置一台用于开发的MacBook","text":"在2022年配置一台用于开发的MacBook 0 Macbook配置与基本信息14-inch MacBook Pro - Space Grey 32GB unified memory Backlit Magic Keyboard with Touch ID - US English 1TB SSD storage 14-inch Liquid Retina XDR display Apple M1 Max with 10-core CPU, 24-core GPU, 16-core Neural Engine Three Thunderbolt 4 ports, HDMI port, SDXC card slot, MagSafe 3 port 1 系统偏好设置1.1 打开电池百分比现在的macOS默认不显示当前电池百分比，需要在系统偏好设置中手动打开 方法：系统偏好 - 程序坞和菜单 - 电池 - 显示电池百分比 1.2 显示蓝牙图标不知道为什么苹果现在默认隐藏了蓝牙图标，需要先点开状态调上的控制中心才能找到蓝牙按钮，直接按住图标将其拖拽到上方状态栏即可。 1.3 文件路径访达并不是那么好用，为了更直接的了解当前文件所在位置，个人建议显示文件的路径 方法：打开终端（Terminal）使用以下命令 12# show path bardefaults write com.apple.finder ShowPathbar -bool true 1.4 访达窗口下方状态栏为了更方便的用鼠标拖动的方式放大/缩小图标，以及查看当前位置的文件个数和状态，可以打开访达窗口下方的状态栏。 方法：打开终端（Terminal）使用以下命令 12# show status bardefaults write com.apple.finder ShowStatusBar -bool true 1.5 触控板触控板可以选择“单指轻触”来代替“按下”，并且自定义触控板速度 位置：系统偏好 - 触控板 1.6 ScreenshotmacOS的截屏很方便，使用默认快捷键command+shift+4即可，会得到一张高清png图，如果希望缩小截图所占空间，可以令系统默认使用jpeg格式。 1defaults write com.apple.screencapture type jpg 1.7 外来软件权限许多在软件官网下载安装包会因为苹果的安全限制无法打开，可以进入：系统偏好设置 - 安全性与隐私 - 通用，允许任意来源软件的安装。 顺便可以解锁让Apple Watch解锁Mac的功能（记得先解锁Apple Watch） Remark：如果找不到“任意来源”，可以使用命令行命令 1sudo spctl --master-disable *此处需要输入PIN 2 终端与命令行2.1 HOMEBREWHomebrew是一款经典的开源软件包管理系统，它可被用来简化macOS系统上的软件安装过程。 1/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" 随后执行以下命令（会默认替换username），将Homebrew添加到环境变量 12echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; /Users/username/.zprofileeval \"$(/opt/homebrew/bin/brew shellenv)\" 使用Homebrew安装以下软件包 1234567brew install \\ wget \\ exa \\ git \\ nvm \\ pnpm \\ vips 2.2 iTerm2 &amp; Oh My Zsh2.2.1 iTerm2macOS自带的终端（Terminal）在灵活度和颜值上都稍有欠缺，在此推荐使用 iTerm2，可以提供全屏编辑/窗口拆分/窗口透明等功能。 ZSH_THEME=”powerlevel10k/powerlevel10k” https://github.com/romkatv/powerlevel10k 2.2.2 Oh My ZshOh My Zsh是一款社区驱动的命令行（终端）工具，可以方便地配置主题，并提供了插件机制（如命令的自动补全、提示等），使用以下命令安装即可 1sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/too 随后，每当Oh My Zsh的配置改变时，都需要执行： 1source ~/.zshrc 我选择的主题是：ZSH_THEME=”powerlevel10k/powerlevel10k”，在~/.zshrc文件中修改对应参数即可。也建议尝试omz的插件，比如 zsh-autosuggestions 等。 地址：https://github.com/romkatv/powe 3 软件推荐接下来就是软件推荐清单了，仅仅是推荐一些个人常用的软件。除了日常应用外，我还选择了一些可能对CS/SE/DS等相关专业学生大裨益的软件。我将他们分成三个类别，并且附上了官网链接。 Remark：如果你和我一样使用的是M1系列芯片（包括M1 Pro 和 M1 Max），在官网下载时请选择apple silicon mac版本的软件 比较Intel与Apple Silicon：https://support.apple.com/en-us/HT211814 3.1 日常使用 腾讯柠檬清理：据说是企鹅为数不多的良心产品，用来清理垃圾文件。 CleanMyMac X：功能更全面的系统垃圾清理软件，缺点是需要购买License。 Snipaste：占用低但功能丰富的截图软件，目前macOS已经有了测试版，免费版即可覆盖大部分人的日常需求。 欧路词典：用了若干年的词典软件，个人是否好用取决于是否构建好适合自己的词典库。 Outlook：用于管理邮件，在App Store下载 The Unarchiver：非常好用的的macOS端解压软件，在App Store下载 MonitorControl：外接显示器亮度调整（GitHub开源） Zoom/Cisco Webex/腾讯会议，各取所需 3.2 笔记与文献管理 Notion：全能型笔记软件，本人为重度用户，利用Notion构建了如电子图书馆，知识库等，有教育优惠。可以使用微信小程序Notion助手方便地通过分享链接保存知乎/微信公众号文章。此外，虽然我还有一年半的印象笔记订阅，但因其愈发过分的广告，我已在迁移出所有内容后将其弃用。 Typora：轻量级Markdown编辑器 Zotero/Mendeley Desktop：用于文献管理，安装插件后可以和Notion搭配使用 Mathpix：用于OCR数学公式，将其转为markdown语法 Goodnotes/Notability：老牌的笔记软件，在App Store下载 Calibre：用于电子书管理 语雀：用于构建/查看知识库、协作编辑，支持多种编辑器（其实网页版的飞书文档也不错） 3.3 开发用途 Filezilla2：FTP传输客户端,用于向服务器（及跳板）传文件 VS Code：无需多言 Jetbrains全家桶（如果是学生身份可以无限白嫖） Sublime Text：跨平台文本编辑器 Navicat：图形化数据库管理工具，用于连接本地和远程数据库 GitHub Desktop：通过图形界面更方便的管理GitHub仓库 Docker：用于容器管理 一些关于 MacBook 的基本配置和软件推荐暂且就先介绍到这里，希望对读到这里的你有所帮助。 Reference https://www.robinwieruch.de/mac-setup-web-development/ https://zhuanlan.zhihu.com/p/48207191","link":"/2022/08/23/2022-8-23-MacSetup/"},{"title":"论文导读：CodeQA","text":"[论文导读] CodeQA: A Question Answering Dataset for Source Code Comprehension","link":"/2022/08/29/2022-8-29-CodeQA/"},{"title":"新的篇章","text":"Turning a new page 距离到新加坡已经过去了超过一周，完成了各种繁琐的手续办理（包括但不限于办学生签、办银行卡选课等等等），本篇博文就简单晒晒最近玩了些什么。 由于来学校第一天的时候选择了步行，被大太阳和学校崎岖的路面好好教育了一顿，因此第二次来就找了校车坐（虽然那时候还没卡，不过shuttle bus的司机也不查），然后因为没想好哪站下就坐到了University Town，主要用来吃饭和使用学校的健身房。UTown北面就是Yale-NUS College，打算趁它还在的时候（Yale-NUS要被合并了）找个时间去参观参观。 在学校的第一顿饭是在工学院食堂吃的（离图书馆很近），然后遇到了一只鸟来陪我一起吃饭。 中央图书馆还是挺大的，不过似乎并不鼓励大家在里面喝东西，因此倒水需要到1、3或6楼才有。 学校在图书馆还很贴心的提供了这种自闭小隔间，可以双人使用。 一楼的COOP（一个类似校园店和生活用品店的结合体）里有卖各种专业的教科书，里面不乏本科期间用过的许多经典教材，本科期间听若干位在NYUSH的同学吐槽国外教材的价格，今天发现比自己想象的还要离谱（或许是国内影印版和盗版用惯了），但不论怎么说，回去的时候还是要买几本喜欢的收藏一下的。 去理学院上课前，在边上的医学院大楼玩了一圈，草坪一如既往的不错，然后偷偷买了杯咖啡倒在保温杯里去乖乖听课了。 教室比较大（而且高），空调开的很厉害，但是因为上课需要戴口罩，所以在里面一坐坐三个小时还是很累的一件事。 坐校车时还经过了学校的李光前自然历史博物馆，不过去的时候有点晚，已经错过了最后入馆的时间，打算过几天再去。 大部分事情安稳下来了以后，我选了个阳光明媚的周末，直接从家门口坐地铁到了滨海金沙的苹果店去逛了一圈。 然后发现自己看中的iPad Pro 12.9 inch + 512g存储（标配是256）正好店里有货，当场就提了一个回去。买的时候这里的银行卡还没办下来，因此直接刷了银联，后面发现汇率比支付宝的实时汇率还贵两分钱… 因此决定一定要等过段时间这里卡办好了再去换MacBook Pro，随后怎么处理教育优惠送的AirPods就是个问题了。 在这里办了两张卡 工行新加坡分行的多币种卡（美元新元和人民币），用来免手续费从国内汇款 DBS，用于日常生活（包括坐车） 工行是去线下walk-in办的，DBS是网上申请的，比预想中晚了一周才拿到卡 最后，作为地标的鱼尾狮显然是要去逛逛的～ 生活正慢慢步入正轨，希望在这里读硕士的一年半中能有所成就。","link":"/2022/08/02/2022-8-A-new-chapter/"},{"title":"My New Hexo Blog","text":"兜兜转转，因为扩展性的原因从Hugo切回了Hexo，前面的一些文章是老博客和Notion主页中的内容的同步。 这个博客的意义博客其实从2020年就开始维护了，但因为在华师大数据学院的学习和生活实在过于繁忙，即使课程和项目方面好不容易闲下来了，时间又被申请季填满，因此其更新频率并不高。就内容而言，我因为考虑到一些扩展性的原因将博客迁回Hexo后，从老博客和自己的Notion主页中筛选了一些内容放在新博客，或许有些杂乱，但也有一定的价值。 博客的内容目前暂定更新以下Topics: 论文学习笔记（主要以深度学习为主），主要涵盖下面几个方面 自然语言处理 代码表征（Code Intelligence） 机器学习系统 少量计算机视觉（科普用/讲述一些idea的来源） … 阅读笔记（或许是教材，或许是科普性质读物） 分享使用LaTeX排版的经验 课程笔记/项目 其他我觉得有意思的东西 和自己对话本科阶段在各种焦虑中度过，自认为学习写作或许会是个情绪的出口。此外，毕业后才渐渐意识到了提升表达和写作能力的重要性，所以，这个博客也就当做一个同时和自己与读者对话的窗口吧。","link":"/2022/07/03/2HEXO/"},{"title":"DaSE Cloud Computing Lecture1","text":"云计算应用与开发：云计算简介及其特征 2020Fall 云计算应用与开发Lecture1 云计算简介云计算发展简史： 云计算概念出现于2000s之前（IBM，AT&amp;T），要点：分布式，虚拟化，Web2.0，面向服务计算，效用计算 云计算落地于2000s（Amazon，Google），要点：云计算技术日益成熟，大量商业模式诞生 云计算的繁荣于2010s（MS Azure，Oracle，OpenStack） 云计算Must Know： 全世界90%的公司在“云上”，其中89%的公司使用SaaS服务 云计算服务三巨头：AWS，Azure，GoogleCloud（57%） 美国拥有全世界最大的公有云计算市场，规模将达1246亿美元，中国第二（105亿美元） 企业使用云服务的最主要原因：随时随地访问数据（42%） 企业使用云服务的最大担忧：数据隐私和数据安全（60%） 企业使用云服务的预算占IT预算比：平均33% 云计算的典型应用：云渲染 云计算的五大基本特征： 按需自助服务（自动化资源分配） 宽带网络访问（Location independent） 资源池化（多租户模式） 快速伸缩（资源快速分配和释放） 可计量（Pay-as-you-go） 云计算的三个核心理念： 虚拟化：提供池化资源和按需服务 多租户：节省开发和维护成本 数据中心：提供基础设施和安全保障 三种经典的云服务模式： SaaS：软件即服务（Softwareas a Service）直接提供应用程序服务，用户无需考虑IT基础设施和应用程序的运维 PaaS：平台即服务（PlatformasaService）为应用程序的开发、部署和管理提供环境，用户无需考虑基础设施的运维 IaaS：基础结构即服务（Infrastructureas a Service）对IT基础结构的直接访问和租用，如服务器、存储空间、网络等等 云计算所面临的挑战： 技术挑战：1.远距离数据传输代价高 2.数据安全与隐私（所有权、不安全接口、数据丢失、黑客攻击）3.数据中心能耗 非技术挑战：1.安全漏洞（数据共享/资源共享）2.不同云平台之间可移植性差（Vendor lock-in） 3.法律问题","link":"/2020/10/16/DaSE-Cloud-Computing%20Lec1/"},{"title":"DaSE Cloud Computing Lecture2","text":"云计算应用与开发：云计算背后的技术 2020Fall 云计算应用与开发Lecture 1 Supplementary VideosGoogle Data Center: https://www.youtube.com/watch?v=X-0V6bYfTpA Microsoft Azure Data Center: https://www.youtube.com/watch?v=yfF3pOzdmlE Lecture2 云计算背后的技术-Part1 数据中心（Data Center）的概念：容纳计算机系统及其相关组件（如通信系统和存储系统）的一组建筑物 最早的数据中心：ENIAC（1940s），重三十吨，占地达167$m^2$ 90年代末大型公司搭建互联网数据中心（Internet Data Center） 云数据中心（Internet Data Center）：2010s 数据中心与云计算云计算基础设施 在不同地理区域建立冗余数据中心保证高可靠和高可用 客户不需要维护本地的数据中心，转而使用IaaS，PaaS，Serverless，SaaS （预计2021年，全球95%的数据中心将会是云计算数据中心） 数据中心的组成 服务器大厅：用于计算，存储和网络设备 冷却场：包含冷却塔和冷却器 电力场：发电机和配电中心 消防系统和安保系统 数据中心中的永久性数据一般放置在storage里面，临时数据可以放在Rack/Servers上 采用N+1冗余，即要N个部件工作的一个系统，额外加一个防止出现意外，完全冗余需要2n+1 数据中心的能耗 数据中心一般常年维持20摄氏度，否则会出现宕机，过热等故障 IT设备和制冷系统消耗大量电能，一次Google搜索会排放7g $CO_2$，两次Google搜索产生的热量可以煮沸一壶水 能源效率 概念：PUE(Power Usage Effectiveness)= 数据中心全部能耗/IT设备能耗，用于衡量冷却、照明等非IT设备的开销比率 理想情况下PUE=1（显然是不可能做到的），美国数据中心平均PUE=2 PUE=1.2即被认为是”Energy Effective Data Center” 可持数据中心（利用可再生资源（太阳能，风能，潮汐能等）） Azure部署水下数据中心 Facebook在北极圈附近部署数据中心 阿里千岛湖数据中心 云计算与虚拟化（云计算依赖虚拟化技术）虚拟化是什么$\\rightarrow$看操作系统 为什么要虚拟化 降低成本（大型主机 Vs. 多台小型主机） 高灵活性（管理多台虚拟机 Vs. 管理多台物理主机） 高敏捷性（虚拟机迁移 Vs. 物理主机迁移） 容错能力强（虚拟节点失效 Vs. 物理节点失效） AWS使用ECS(Elastic Container Service)，Azure使用Nested Virtualization 多租户技术（Multitenancy）多租户技术指的是一种软件架构，使得同一个应用程序可以同时服务多个独立的租户（一个租户通常包括一群权限共享的用户），不同租户享有专用的数据、配置和功能等等 多租户技术显著的的一个好处：app维护的时候一次性维护就够了（易维护）多租户技术的优点： 节约成本（资源池化） 快速启动（不为单独一个租户启动一个应用） 高可扩展性（有限的资源支持更多的租户） 安全性（数据加密、访问控制、多重验证等） 易维护性（一次维护、更新或升级服务所有租户） 多租户系统可以在不同层实现: 应用层：隔离不同租户的应用程序环境（应用的不同进程隔离，进程的不同线程隔离） 系统层：计算资源的隔离（虚拟机、虚拟网络） 数据层：使用不同的方法将不同租户的数据隔离 分布式系统（云本质上是一个分布式系统）不同计算机利用网络互联，通过消息传递沟通并协同完成任务的系统（电信网络，基于网络的应用，实时处理系统。各种并行计算系统） 云通过网络连接 不同功能的服务器互相传递消息、协同工作 • 客户端与服务器传递消息、使用服务 • 不同客户端互相传递消息、共享服务 分布式系统的特性与云计算你会知道系统中某台计算机崩溃了，但是你的软件确永远不会。 ​ ——乔治·库鲁里斯 容错性：单一的故障不会影响整个系统的正常运行，比如冗余、故障恢复。（高可靠&amp;高可用） 高可扩展性：自由对节点或功能进行扩充（弹性伸缩） 并发：多台机器同时访问同一数据 透明性：用户无需了解分布式系统内部结构 分布式计算（Distributed Computing）：分布式存储（Distributed data store）：通过网络接连将数据存储在多个计算机节点上，通常用一种复制（多副本）的方式。 分布式资源管理（ Distributed Resource Scheduling ）：集群管理/容器管理 重复：云计算的三大核心技术： 数据中心 虚拟化 多租户","link":"/2020/10/19/DaSE-Cloud-Computing%20Lec2/"},{"title":"Cloud-Computing Lecture3","text":"云计算应用与开发：云计算背后的技术-2 云计算应用与开发Lecture3 云计算背后的技术-2 Preview： 目前工业界比较强的需求：面向AI的计算 Amazon Elastic Computing 分布式存储的好处（存储与计算的结合） 扩展性能的需求（scale-up vs scale-out） Scale-up(Vertical Scaling)：现有系统中加入更多资源 e.g：更多CPU，内存，更快的网络 Scale-out(Horizontal Scaling)：现有系统加入更多节点 e.g：更多主机节点和存储节点 总结：Scale-up 靠增加处理器来提升运算能力，Scale-out 增加独立服务器来增加运算能力。Scale-up到上限时，就需要Scale-out来满足进一步的需求了 了解：AWS Auto Scaling 分布式计算（Distributed Computing）顺序结构；分支结构；循环结构 MapReduce（Google：用什么处理这么海量数据？Map和Reduce！） MapReduce的概念：由Jeff Dean和Sanjay Ghemawat在2004年在 MapReduce: Simplified data processing on large clusters 中提出Source: http://nil.csail.mit.edu/6.824/2015/papers/mapreduce.pdf MapReduce的起源：数据爆炸，计算非常耗时，数据处理必须依赖分布式计算并行化计算 并行化计算 分布式部署数据 负载均衡 容错机制 而MapReduce将分布式计算抽象化，隐藏上述实现细节 Map Function：将原始数据转化为key/value pair Reduce Function：根据key聚合value，得到最后的计算结果 MapReduce：Dataflow Input reader：读入文件并划分成固定大小的存储块（split，通常64M或128M），并分配到maper节点 Map：将split中的数据根据问题的需求转换为key/value pair并输出 Partition：将map函数的输出结果，根据key值分配到不同的reducer节点 Comparison：对分配到每个reducer节点key/value pair进行排序 Reduce：根据问题需求对每个key的value进行迭代计算，输出key对应的最终结果 Output Writer：将reducer的输出结果写入磁盘文件系统 MapReduce理念：任务通过切分数据的方式，完成小任务，最后再合并 可行：找海量数据中的最大值 Map执行时候，各个Map操作之间是不需要通讯的 MapReduce的应用： 统计URL访问频率 map: &lt;URL, 1&gt;; reduce: &lt;URL, totalCount&gt; 反向Web链接图 map: &lt;targePage, sourcePage&gt;; reduce: &lt;targetPage, list (sourcePage)&gt; 倒排索引 map: &lt;word, docID&gt;; reduce: &lt;word, list (docID)&gt; Hadoop：分布式大数据计算的开源软件集合Hadoop出现后，才叫做“大数据”！Hadoop集群的使用请见Lab","link":"/2020/10/20/DaSE-Cloud-Computing%20Lec3/"},{"title":"DaSE Cloud Computing Lecture4","text":"云计算应用与开发：云计算背后的技术-2 Cont’d 2020Fall 云计算应用与开发Lecture3 云计算背后的技术-2 Cont’d其他分布式计算模型：MPI(Message Passing Interface 消息传递接口) 需要程序员编写点对点收发消息的操作 基于消息传递协议的并行编程接口 比Hadoop/Spark更加灵活 MPI可以实现MapReduce的功能，但反之不然 缺少完善的容错机制，需要手动编写 Reading：https://mpi4py.readthedocs.io/en/stable/ 其他分布式计算模型：BSP(Bulk Synchronous Parallel 整体同步并行计算)以superstep为单位的大同步并行计算 并发计算：并行本地内存计算 通信：消息和数据交换 路障同步：等待所有并行计算完成 内存存储vs磁盘存储？newsql 分布式存储（Distributed Storage System）通过网络接连将数据存储在多个计算机节点上，通常用一种复制（多副本）的方式 分布式存储的类型： 结构化存储（分布式数据库）：MySQL / PostgreSQL (GreenPlum) 非结构化存储：HDFS (Hadoop Distributed File System) / GFS (Google File System) 半结构化存储：Bigtable / Hbase / Dynamo 内存存储：Redis / MonetDB NewSQL：Google Spanner 分布式存储的关键技术 数据一致性：不同副本的数据应当保持一致 数据的均匀分布：不同服务器上数据应分布均匀 容错机制：自动迁移和恢复数据，不要让错误和故障对数据的影响扩散太大，最好只影响自己 负载均衡：将工作量分不到多个服务器上，单台机器上不合理的工作量会拖累整个系统 并发控制：分布式事务处理 CAP定理：C(consistence)A(availability)P(partition tolerance) 分区容忍指的是网络分区 CAP指的是各指标之间的的tradeoff CAP不可能三个同时满足 数据库架构： 了解Greenplum，Greenplum数据库架构相对于传统单机RDBMS可扩展性增强，可用性提高 关系型数据库约束了可扩展性 NoSQL：Not Only SQL，非关系型数据库管理系统，通常被认为是半结构化存储，高可用易扩展，不支持复杂的查询（e.g join） 数据库的强一致性：在任何时刻所有的用户或者进程查询到的都是最近一次成功更新的数据。 最终一致性：在某一时刻用户或者进程查询到的数据可能会不同，但是最终成功更新的数据都会被所有用户或者进程查询到 弱一致性：数据更新后，用户或者进程访问到部分或者全部访问不到最新的更新","link":"/2020/10/22/DaSE-Cloud-Computing%20Lec4/"},{"title":"DaSE Cloud Computing Lecture5","text":"云计算应用与开发：云计算背后的技术-2 Cont’d 2020Fall 云计算应用与开发Lecture5 云计算背后的技术-2 Cont’dKey-Value Storage 键值存储 淘宝 item number $\\rightarrow$ information about the item 携程 flight number $\\rightarrow$ information about the flight Weibo id $\\rightarrow$ all posts of the ID API:gt(key) and put(key,value) Remarks: value可能是xml,json,文档… 为什么要使用Key-Value Storage 海量非结构化数据 大量随机读写 foreign key和join很少使用 Key-Value Storage的特性 非结构化存储 每一行的column不同 不一定要支持foreign key和join 存储方式 row store：表中每一行存储在连续的地址空间，适合处理大量attribute和record，通常采用schema，RDBMS（关系型数据库）常使用row store column store：表中每一列存储在连续的地址空间，适合少量attribute的range search，通常采用key-value形式 NoSQL所面临的挑战 SQL支持不足：往往不支持join等复杂查询 开源：没有统一标准，系统尚不成熟 功能不够丰富：通常用于随机读写非结构化数据，缺乏丰富函数的支持 文件容量通常比较大：JSON，Graph 常见的云计算平台中的NoSQL服务：Google Cloud Datastore，Amazon DynamoDB，Azure Cosmos DB NewSQL：SQL和NoSQl的结合 关系型数据库 遵循ACID原则 满足NoSQL高可扩展性 Remarks:ACID原则是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity）、一致性（consistency）、隔离性（isolation）、持久性（durability）。(Source:wikipedia) Google 三件套：GFS、BigTable、MapReduceHDFS/GFS非结构化存储分布式文件管理系统： 用来存储海量非结构化数据，如日志、网页、图片等 GFS（Hadoop Distributed File System即为GFS的开源实现） GFS vs HDFS：https://www.slideshare.net/YuvalCarmel/gfs-vs-hdfs HDFS的架构 HDFS 的特性$\\rightarrow$ append only!读数据占多，很少去更新已经存在的文件，新数据仅仅是写在老数据后面，顺序地进行写入","link":"/2020/11/04/DaSE-Cloud-Computing%20Lec5/"},{"title":"DaSE Cloud Computing Lecture6","text":"云计算应用与开发：云计算背后的技术-3 2020Fall 云计算应用与开发Lecture6 分布式资源管理概念：SIMD(Single Instruction Multiple Data)，采用一个控制器来控制多个处理器，同时对一组数据中的每一个分别执行相同操作实现空间上的并行 分布式数据存储：数据结构的角度 结构化数据存储 半结构化数据存储 非结构化数据存储 分布式数据存储：数据抽象的角度（数据如何被组织在存储设备中） 块存储（Block-level Storage） 文件存储（Block-level Storage） 对象存储（Object-level Storage） 数据被存储在原始的磁盘卷（裸磁盘），文件被分割成固定大小的“块”，以多副本形式存储在磁盘之中。每个块被赋予唯一的ID，OS通过ID存取数据，读取文件时文件系统重新组装数据块。数据和用户环境脱钩，可以分布在多个不同操作系统环境中，可以有多个存储路径。 在云环境中，数据块通过SAN(Storage Area Network)被存放在最合适的位置 块存储的特点和应用优点： 延时低，高效，高容错 数据可以跨平台存储 适合处理企业级海量数据 缺点： 缺乏元数据处理机制（通常由操作系统代劳） 价格昂贵，SAN引入了额外开销 数据共享不便 应用：数据库，RAID，虚拟机存储系统 文件存储的特点和应用文件存储是传统的操作系统存储方式，依赖于特定的文件系统（如FAT32，Ext4，NTFS），不同块的数据被组织成文件，文件按照层级目录结构组织，依靠路径对文件进行定位。文件存储通常用于本地硬盘和网络附加存储（NAS，Network Attached Storage）设备 优点： 层次结构简单清晰 文件量不多时定位快 使用NAS可以实现scale-out与多人共享 缺点： scale-up困难，一般只能替换存储设备 从海量文件中定位文件相对较难 应用：各类型文件的共享和存档，文件备份和灾备 对象存储的特点和应用对象存储用于存储海量非结构化数据，不同块中的数据连同元数据和唯一ID被打包为对象，OSD（Object Storage Device）将所有对象组织在扁平地址空间中（Flat Address Space），即所有对象都存储在同一层级，应用通过HTTP API（RESTful）来访问对象存储系统 优点： 可以包含非常丰富的元数据 适合分布式存储，可扩展性高（无参差结构） 通过调用API访问，支持Pay-as-you-go 缺点： 读写效率相较块存储和文件存储低 对象不易修改，不支持频繁修改，如事务处理 应用：非结构化数据存储，数据分析，网页应用，备份存档 下图为三种存储方式的总结 三种存储方式在云计算中的典型产品 块存储：Amazon Elastic Block Storage (EBS), Google Persistens Disks 文件存储：Amazon Elastic File System (EFS), Azure File 对象存储：Amazon Simple Storage Service (S3), Google Cloud Storage, Azure BLOB Storage BLOB: 二进制大型对象（英语：Binary Large Object ，或英语：Basic Large Object，缩写为Blob、BLOB、BLOb），在DBMS中，将二进制资料存储为一个单一个体的集合。Blob通常是影像、声音或多媒体文件。(source: wikipedia)","link":"/2020/11/22/DaSE-Cloud-Computing%20Lec6/"},{"title":"华东师大本科毕业设计模版（Class of 2022）","text":"华东师范大学本科毕业论文（设计）的TeX模版已经好几年没人更新了，为了能保持排版规范和一致性，我对老LaTeX模版中的一些细节进行了修正，并对应word模版修改了层次结构，再添加了一些新功能。 话不多说，先上项目地址：QiushiSun-ECNU-Undergraduate-Thesis-Template-2022 1. 基于老模版的修改本模板为了迎合2018级（2022届）华东师范大学本科生毕业（论文）设计的模板需要，基于 YijunYuan/ECNU-Undergraduate-LaTeX 和 Koyamin/ECNUThesis-Undergraduate 进行了修改，让同学们可以最大限度地节约排版时间。 修改细节包括 修改了页眉的文章title说明 修改了封面、中英文摘要的字体 支持插入（更漂亮的）代码块 新增了插入图片的Bicaption以及表格的Bicaption，提供中英文文字说明 重新设置了TOC(Table Of Contents) 的深度 调整 itemize 环境中的条目之间的间距 使用了gb7714-2015引用规范 更新 2022/3/30：添加了生僻字支持，字体地址：https://github.com/micmro/Stylify-Me/blob/master/.fonts/SimSun.ttf （若出现了生僻字输出错误再安装该字体） 2022/4/8：修正了section/subsection/subsubsection后出现多余”.”的问题，并添加了此部分示例 2022/4/17：在目录前添加了诚信承诺书，在使用时把Integrity.pdf替换为签名好的pdf文件（命名为Integrity.pdf）即可。 2022/4/17：修改了四级标题为*.*.*.*形式，与word模板保持一致。 2022/4/20：修改了表格Caption的位置，与word模板保持一致。 2022/4/22：调整了页边距，上：2.5cm，下：2.0 cm，左：3.0cm，右：2.5 cm以与word模板保持一致，修正了引用会议论文时缺失地址导致出现[S:n]占位的情况。 说明Attention is all You need! 本模版只能使用 XeLaTeX 进行编译，使用其它 TeX 引擎将会导致不可避免地编译失败或是输出异常。 一般情况下，不需要修改.cls文件 使用环境使用 XeLaTeX2020 和 XeLaTeX2021 编译器均能编译成功，本地配置环境有困难但网络环境稳定的同学建议使用Overleaf。 2. 如何在iPad端轻松加愉快地使用善用PWA功能（它本质上还是个浏览器），首先在Safari浏览器中打开Overleaf，选中项目 点击分享按钮，再点击“添加至桌面” 就可以愉快的玩耍了","link":"/2022/03/28/LaTeX-Template-for-ECNU-undergraduate-thesis/"},{"title":"The Craft of Research Part1","text":"[读书笔记] 研究的艺术：与读者建立联系","link":"/2022/07/05/The-Craft-of-Research-Part1/"},{"title":"(Xe)LaTeX小技巧","text":"分享一些在配置毕业论文模版时发现的(Xe)LaTeX小技巧 最近在配置2022年的毕业论文 LaTeX 模版，在GitHub上找了多个老模版，都无法满足今年的文章格式要求（参考了教务老师提供的Word模版），因此分享一些在配置模版中不容易发现的小技巧。 1. 控制目录的深度如题所示，LaTeX中的TOC(Table Of Contents) 深度可以依据你的需求来控制深度，默认为Chapter + section + subsection，如下 可以使用命令 1\\setcounter{tocdepth}{1} 来缩减目录的深度 这个条目中的参数对应的目录深度如下 Param Depth -1 part 0 chapter 1 section 2 subsection 3 subsubsection 4 paragraph 5 subparagraph 2. 调整Itemize项之间的间距$\\LaTeX$ 默认的itemize条目项间距很大，插入下列itemize代码 1234\\begin{itemize} \\item Microsoft CodeXGLUE任务数据集，其包含了xx条从GitHub的开源仓库中所采集的函数 \\item 华东师范大学数据科学与工程学院水杉码园数据集，其包含了约100,000个由数据学院学生在Online Judge平台上提交的代码片段。\\end{itemize} Itemize的效果如下所示 使用以下命令来修改itemize的间距 1234\\usepackage{enumitem}\\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\\parskip,topsep=5pt}\\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\\parskip,topsep=5pt}\\setdescription{itemsep=0pt,partopsep=0pt,parsep=\\parskip,topsep=5pt} 查看缩减行间距后的效果 3. 为图片添加双语说明当前的毕业论文模版要求每张插图都需要双语Caption（虽然我不懂这是为什么…），但既然要求如此，那就需要在一般的Caption上做一些修正 需要先导入一个额外的包 12\\usepackage{bicaption} %caption的扩展package\\captionsetup[figure][bi-second]{name=Figure}%将英文图的开头设定为字符“Figure” 随后插图 12345\\begin{figure}[H] \\centering \\includegraphics[scale=yourscale]{source/pic.png} \\bicaption{卷积神经网络（忽略补全），循环神经网络和自注意力机制的比较}{Comparing CNN (padding tokens are omitted), RNN, and self-attention architectures}\\end{figure} 通过下图查看Bicaption的效果 如果有其他的新发现就会继续更新！","link":"/2022/03/19/LaTeX-Tricks-undergraduate-thesis/"},{"title":"The Craft of Research Part3","text":"[读书笔记] 研究的艺术：如何讲好故事与论点","link":"/2022/07/16/The-Craft-of-Research-Part3/"},{"title":"The Value of Research &amp; Novelty in Science","text":"学习笔记：判断研究工作的价值，以及研究新意度（Novelty）的五大误解 本篇博客是基于李沐老师的视频： 如何判断（你自己的）研究工作的价值 你（被）吐槽过论文不够 novel 吗？ 总结的学习笔记 研究工作的价值核心思想：用有新意的方法有效地解决一个研究问题 问题 工程问题 如算法很占内存，代码是否能优化，实现方式是否能优化 如模型精度不高，那是否能多采样点数据 研究问题 有效（相对地有效性） 相对之前的工作，你一般无法彻底解决一个问题，相对之前的工作我是否能做提升 新意（Novelty）：对你的研究社区中的其他研究者而言，你用的方法别人没想到 研究工作的价值新意度 x 有效性 x 问题大小 = 价值 每一项都可以分为三个级别：1，10，100 新意度 有效性 问题大小 1（说了你的方法，后面的文章结果都不用看了） 1（好那么一点点，多重复几次就差不多了） 1（对前面的工作在不好一个的点改进） 10（某一方面用了聪明的技术） 10（中等，可见的提升） 10（比如一个计算机视觉的子任务） 100（之前的工作没用过，打开了新世界的大门） 100（一个工作就能向前推5-10个点） 100（可以大到解决通用的人工智能） 如果一个工作能拿到两个10，那么值得一写，如果能拿到10，10，10就很不错，两个方向拉满就非常困难，三个方向都拉满即功成名就。 研究新意度（Novelty）的五大误解文章链接：Novelty in Science，这篇文章是用来教育reviewer的。 作者背景：德国马普所智能系统研究院的创始人之一，研究方向是计算机视觉、计算机图形学等。 用来评价论文价值的公式对强理论性的工作并不适用，因为对一个理论问题来说，你很难判断他这个领域的大小，有效性也不是很好判断，因为一个定理总是正确的。所以理论研究者对一个文章的判断，作者经常用两个词： 深刻：你是不是揭示了一些本质的一些东西 优美：你的定理的本身和你证明，是不是有美感（与个人品味相关） 优美这个词的好处是他把技术性和复杂性从指标中剥夺出来了 参考：毕加索和伦勃朗，我们都能欣赏出美来，即使创作过程作品的复杂度有区别，创作的艰难度上有区别。 1. Novelty as Complexity：用复杂度衡量NoveltyThe simplicity of an idea is often confused with a lack of novelty when exactly the opposite is often true. A common review critique is The idea is very simple. It just changes one term in the loss and everything else is the same as prior work. If nobody thought to change that one term, then it is ipso facto novel. The inventive insight is to realize that a small change could have a big effect and to formulate the new loss. 典型：只修改了损失函数 2. Novelty as Difficulty：用困难度衡量NoveltyFormulating a simple idea means stripping away the unnecessary to reveal the core of something. This is one of the most useful things that a scientist can do. 典型：Ian Goodfellow的GAN，着重解释对抗的思想，而不是为了效果堆砌复杂技术（比如把文中的MLP替换为CNN） 3. Novelty as Surprise：用惊讶来衡量Novelty完全使用Surprise来衡量，那么会认为一个新的工作自然而然，人的思维就是这样的。 The idea is obvious because the authors just combined two well known ideas. 在你没有听到前，你大概率没想到过！ 典型：MAE，就是简单的技术拼在一起，但是在这之前并没有人想到，所以它拥有Novelty。 4. Novelty as Technical Novelty：用技术新意来等价新意典型：ImageNet，大家都是晚上抓图片然后人工标注，但是ImageNet大啊，大才能支撑深度学习中的各种算法。 此外，如果你把一个复杂的算法替换为简单的算法，而且能提供一些Insight，那当然也是有效的。 5. Novelty as Usefulness or value：用有效性等价新意Lack of utility is indeed an issue but it is very hard to assess with a new idea. Reviewers should be careful here and aware that we all have limited imagination. 实用性的缺少不能代表新意度低，它的确是个问题，但是后面可能会被其他研究者玩出花来 总结：Novelty≠ Complexity, Difficulty, Surprise, Technical Novelty, Usefulness.文章作者认为新意度，更多的是关于这个想法是不是优美的。","link":"/2022/02/06/Novelty%20in%20Science/"},{"title":"The Craft of Research Part2","text":"[读书笔记] 研究的艺术：阐述问题的重要性","link":"/2022/07/08/The-Craft-of-Research-Part2/"}],"tags":[{"name":"TeX","slug":"TeX","link":"/tags/TeX/"},{"name":"PTMs","slug":"PTMs","link":"/tags/PTMs/"},{"name":"CodeRep","slug":"CodeRep","link":"/tags/CodeRep/"},{"name":"Interpretability","slug":"Interpretability","link":"/tags/Interpretability/"},{"name":"ML-System","slug":"ML-System","link":"/tags/ML-System/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"Multimodality","slug":"Multimodality","link":"/tags/Multimodality/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"misc","slug":"misc","link":"/tags/misc/"},{"name":"QA","slug":"QA","link":"/tags/QA/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"cloud-computing","slug":"cloud-computing","link":"/tags/cloud-computing/"},{"name":"DaSE@ECNU","slug":"DaSE-ECNU","link":"/tags/DaSE-ECNU/"},{"name":"writing","slug":"writing","link":"/tags/writing/"},{"name":"research","slug":"research","link":"/tags/research/"}],"categories":[{"name":"TeX","slug":"TeX","link":"/categories/TeX/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"ML-System","slug":"ML-System","link":"/categories/ML-System/"},{"name":"Blog","slug":"Blog","link":"/categories/Blog/"},{"name":"CodeRep","slug":"NLP/CodeRep","link":"/categories/NLP/CodeRep/"},{"name":"Multimodality","slug":"Multimodality","link":"/categories/Multimodality/"},{"name":"misc","slug":"misc","link":"/categories/misc/"},{"name":"DaSE@ECNU","slug":"DaSE-ECNU","link":"/categories/DaSE-ECNU/"},{"name":"cloud-computing","slug":"DaSE-ECNU/cloud-computing","link":"/categories/DaSE-ECNU/cloud-computing/"},{"name":"Research","slug":"Research","link":"/categories/Research/"},{"name":"research","slug":"research","link":"/categories/research/"},{"name":"Writing","slug":"Research/Writing","link":"/categories/Research/Writing/"}],"pages":[{"title":"","text":"友链 本站友链信息如下，欢迎友链： 网站图标：Avatar 网站名称：Qiushi’s Blog 网站地址：https://qiushisun.github.io 加载中，稍等几秒...","link":"/friend/index.html"}]}